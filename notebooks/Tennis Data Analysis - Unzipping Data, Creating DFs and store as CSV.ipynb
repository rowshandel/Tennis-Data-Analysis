{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the main directory where the zip file is located\n",
    "main_zip_path = r\"E:\\Data Analysis Course\\Data analyse\\Data Analyse final project\"\n",
    "file_name = '202405.zip'\n",
    "file_path = os.path.join (main_zip_path, file_name)\n",
    "\n",
    "# Path to the target extraction directory\n",
    "extract_to = r\"E:\\Data Analysis Course\\Data analyse\\Data Analyse final project\\extracted\"\n",
    "\n",
    "# Ensure the target extraction directory exists\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# Unzip the main folder to the target directory\n",
    "with zipfile.ZipFile(file_path,'r') as zip_ref:\n",
    "    zip_ref.extractall (extract_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, list the directories inside the extracted folder\n",
    "extracted_files = os.listdir(extract_to)\n",
    "# Loop through each extracted file to unzip its contents if they are zip files\n",
    "for file in extracted_files:\n",
    "    file_path = os.path.join(extract_to, file)\n",
    "    if zipfile.is_zipfile(file_path):\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            # Extract each zip file to a directory named after the zip file (without the .zip extension)\n",
    "            sub_extract_path = os.path.join(extract_to, file.replace('.zip', ''))\n",
    "            os.makedirs(sub_extract_path, exist_ok=True)\n",
    "            zip_ref.extractall(sub_extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SilkRoadit.com\\AppData\\Local\\Temp\\ipykernel_39184\\3982440912.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_dfs = {schema: pd.concat(dfs, ignore_index=True) for schema, dfs in schema_dfs.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatchAwayTeamInfo DataFrame Shape: (12955, 18)\n",
      "MatchAwayScoreInfo DataFrame Shape: (19676, 14)\n",
      "MatchEventInfo DataFrame Shape: (19676, 10)\n",
      "MatchHomeTeamInfo DataFrame Shape: (14090, 18)\n",
      "MatchHomeScoreInfo DataFrame Shape: (19676, 14)\n",
      "MatchRoundInfo DataFrame Shape: (12087, 5)\n",
      "MatchSeasonInfo DataFrame Shape: (19676, 4)\n",
      "MatchTimeInfo DataFrame Shape: (19676, 7)\n",
      "MatchTournamentInfo DataFrame Shape: (19676, 16)\n",
      "MatchVenueInfo DataFrame Shape: (19589, 5)\n",
      "OddsInfo DataFrame Shape: (33740, 11)\n",
      "GameInfo DataFrame Shape: (1467013, 13)\n",
      "PeriodInfo DataFrame Shape: (794368, 13)\n",
      "PowerInfo DataFrame Shape: (269694, 5)\n",
      "MatchVotesInfo DataFrame Shape: (19677, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold DataFrames for each schema\n",
    "schema_dfs = {}\n",
    "\n",
    "# Traverse the directories to find Parquet files and assign them to appropriate schemas\n",
    "\n",
    "# 1- finding parquet files and creating file_path for each parquet file:\n",
    "for root, dirs, files in os.walk(extract_to):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            schema_name = None\n",
    "            # 2- Determine schema based on the file name\n",
    "            if file.startswith('away_team_1'):\n",
    "                schema_name = 'MatchAwayTeamInfo'\n",
    "            elif file.startswith('away_team_score'):\n",
    "                schema_name = 'MatchAwayScoreInfo'\n",
    "            elif file.startswith('event'):\n",
    "                schema_name = 'MatchEventInfo'\n",
    "            elif file.startswith('home_team_1'):\n",
    "                schema_name = 'MatchHomeTeamInfo'\n",
    "            elif file.startswith('home_team_score'):\n",
    "                schema_name = 'MatchHomeScoreInfo'\n",
    "            elif file.startswith('round'):\n",
    "                schema_name = 'MatchRoundInfo'\n",
    "            elif file.startswith('season'):\n",
    "                schema_name = 'MatchSeasonInfo'\n",
    "            elif file.startswith('time'):\n",
    "                schema_name = 'MatchTimeInfo'\n",
    "            elif file.startswith('tournament'):\n",
    "                schema_name = 'MatchTournamentInfo'\n",
    "            elif file.startswith('venue'):\n",
    "                schema_name = 'MatchVenueInfo'\n",
    "            elif file.startswith('odds'):\n",
    "                schema_name = 'OddsInfo'\n",
    "            elif file.startswith('pbp'):\n",
    "                schema_name = 'GameInfo'\n",
    "            elif file.startswith('statistics'):\n",
    "                schema_name = 'PeriodInfo'\n",
    "            elif file.startswith('power'):\n",
    "                schema_name = 'PowerInfo'\n",
    "            elif file.startswith('votes'):\n",
    "                schema_name = 'MatchVotesInfo'\n",
    "\n",
    "            # If schema is identified, add the file to the corresponding DataFrame list\n",
    "\n",
    "            # 3- Creating a list of schema names named schema_dfs\n",
    "            if schema_name:\n",
    "                if schema_name not in schema_dfs:\n",
    "                    schema_dfs[schema_name] = []\n",
    "            # 4- reading each parquet file with file_path and putting in dataframe named df\n",
    "                df = pd.read_parquet(file_path)\n",
    "            # 5- Creating the schema_dfs dictionary which keys are schema_names and values are dfs\n",
    "                schema_dfs[schema_name].append(df)\n",
    "\n",
    "# 6- Concatenate DataFrames for each schema\n",
    "final_dfs = {schema: pd.concat(dfs, ignore_index=True) for schema, dfs in schema_dfs.items()}\n",
    "\n",
    "# 7- Display final DataFrames\n",
    "for schema, df in final_dfs.items():\n",
    "    \n",
    "    print(f\"{schema} DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatchAwayTeamInfo = final_dfs ['MatchAwayTeamInfo']\n",
    "MatchAwayScoreInfo = final_dfs ['MatchAwayScoreInfo']\n",
    "MatchEventInfo = final_dfs ['MatchEventInfo']\n",
    "MatchHomeTeamInfo = final_dfs ['MatchHomeTeamInfo']\n",
    "MatchHomeScoreInfo = final_dfs ['MatchHomeScoreInfo']\n",
    "MatchRoundInfo = final_dfs ['MatchRoundInfo']\n",
    "MatchSeasonInfo = final_dfs ['MatchSeasonInfo'] \n",
    "MatchTimeInfo = final_dfs ['MatchTimeInfo']\n",
    "MatchTournamentInfo = final_dfs ['MatchTournamentInfo']\n",
    "MatchVenueInfo = final_dfs ['MatchVenueInfo']\n",
    "OddsInfo = final_dfs['OddsInfo']\n",
    "GameInfo = final_dfs ['GameInfo']\n",
    "PeriodInfo = final_dfs ['PeriodInfo']\n",
    "PowerInfo = final_dfs ['PowerInfo']\n",
    "MatchVotesInfo = final_dfs ['MatchVotesInfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatchAwayTeamInfo.to_csv ('MatchAwayTeamInfo.csv', index=False)\n",
    "MatchAwayScoreInfo.to_csv ('MatchAwayScoreInfo.csv', index=False)\n",
    "MatchEventInfo.to_csv ('MatchEventInfo.csv', index=False)\n",
    "MatchHomeTeamInfo.to_csv ('MatchHomeTeamInfo.csv', index=False)\n",
    "MatchHomeScoreInfo.to_csv ('MatchHomeScoreInfo.csv', index=False)\n",
    "MatchRoundInfo.to_csv ('MatchRoundInfo.csv', index=False)\n",
    "MatchSeasonInfo.to_csv ('MatchSeasonInfo.csv', index=False)\n",
    "MatchTimeInfo.to_csv ('MatchTimeInfo.csv', index=False)\n",
    "MatchTournamentInfo.to_csv ('MatchTournamentInfo.csv', index=False)\n",
    "MatchVenueInfo.to_csv ('MatchVenueInfo.csv', index=False)\n",
    "OddsInfo.to_csv ('OddsInfo.csv', index=False)\n",
    "GameInfo.to_csv ('GameInfo.csv', index=False)\n",
    "PeriodInfo.to_csv ('PeriodInfo.csv', index=False)\n",
    "PowerInfo.to_csv ('PowerInfo.csv', index=False)\n",
    "MatchVotesInfo.to_csv ('MatchVotesInfo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
